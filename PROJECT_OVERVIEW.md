# HyperVAE: Bipartite Hypergraph Variational Autoencoder for Jet Generation

## Project Overview

**HyperVAE** is a physics-informed deep generative model for synthesizing particle physics jets using Variational Autoencoders (VAE) with Lorentz-equivariant transformations. The model generates jets as bipartite hypergraphs, preserving both the underlying physics symmetries and complex multi-particle correlations.

### What are Jets?

In particle physics, **jets** are collimated sprays of particles produced in high-energy collisions (e.g., at the Large Hadron Collider). They are the experimental signatures of quarks and gluons, which cannot be observed directly. Understanding and simulating jets is crucial for:

- Detecting new particles (Higgs boson, supersymmetry, etc.)
- Measuring Standard Model parameters
- Rejecting background events in particle searches
- Calibrating particle detectors

### Why Generative Models for Jets?

Traditional physics simulations (e.g., PYTHIA, GEANT) are:
- **Slow**: Can take minutes per event
- **Computationally expensive**: Require massive computing clusters
- **Fixed**: Hard to modify underlying physics assumptions

**Deep generative models** offer:
- **Speed**: Generate jets in milliseconds (1000Ã— faster)
- **Flexibility**: Learn directly from data
- **Interpolation**: Explore phase space efficiently
- **Data augmentation**: Increase training samples for ML applications

---

## Key Features

### 1. **Lorentz Equivariance (L-GATr)**

HyperVAE uses **L-GATr (Lorentz Group Attention)** to ensure generated jets respect special relativity:

- **Input**: Particle 4-vectors `[E, px, py, pz]` (energy + 3-momentum)
- **Transformation**: Geometric Algebra operations preserving spacetime symmetries
- **Output**: Physically consistent particles satisfying `EÂ² = pÂ² + mÂ²`

**Benefit**: Generated jets obey fundamental physics laws (Lorentz boosts, rotations) automatically.

### 2. **Bipartite Hypergraph Representation**

Jets are represented as graphs with:
- **Particles**: Primary objects (nodes)
- **Edges**: Pairwise particle relationships with 2-point correlations (2-pt EEC, angular distance, etc.)
- **Hyperedges**: Higher-order correlations (3-point, 4-point structures)

Note: Edge and hyperedge features are pre-computed from the dataset during graph construction. The decoder generates only particles; edge/hyperedge observables are computed from particles for distribution matching during training.

**Benefit**: Captures complex jet substructure while keeping generation tractable.

### 3. **Squared Distance Chamfer Loss**

Novel loss function for stable training:

```
DÂ²(i,j) = w_energy Ã— (E_i - E_j)Â² + (px_i - px_j)Â² + (py_i - py_j)Â² + (pz_i - pz_j)Â²
```

**Key Innovation**: Squared distance has **linear gradients** (âˆ‚dÂ²/âˆ‚x = 2x) vs. vanishing Euclidean gradients (âˆ‚âˆšdÂ²/âˆ‚x = 1/(2âˆšx)).

**Benefit**: Prevents training plateau, faster convergence.

### 4. **Memory-Optimized for Consumer Hardware**

Designed to run on **4GB VRAM** (e.g., GTX 1650 Ti):
- Gradient accumulation (effective batch size 256)
- Multi-query attention (reduces memory)
- Efficient PyG batch format
- Optional gradient checkpointing

**Benefit**: Train competitive models without expensive GPUs.

### 5. **Distribution Matching**

Rather than directly generating edge/hyperedge features, the model:
- Generates particles using L-GATr
- Computes observables from particles during training
- Matches distributions with pre-computed dataset features using Wasserstein distance

**Benefit**: Simpler architecture, better particle quality, physics-consistent observables.

---

## Model Architecture

```
Input Jets (PyG Data)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ENCODER                           â”‚
â”‚  â€¢ L-GATr Particle Encoding                â”‚
â”‚  â€¢ Edge/Hyperedge MLPs                      â”‚
â”‚  â€¢ Cross-Attention Fusion                   â”‚
â”‚  â€¢ Global Pooling                           â”‚
â”‚  â†’ Latent Distribution q(z|x) = N(Î¼, ÏƒÂ²)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
  z ~ N(Î¼, ÏƒÂ²)  [Reparameterization Trick]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DECODER                           â”‚
â”‚  â€¢ Latent Expansion + Jet Type Conditioningâ”‚
â”‚  â€¢ Topology Decoder (Gumbel-Softmax)       â”‚
â”‚  â€¢ L-GATr Particle Generation              â”‚
â”‚  â€¢ Jet Feature Prediction                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Generated Jets
```

**For detailed architecture**, see [MODEL_ARCHITECTURE.md](MODEL_ARCHITECTURE.md).

---

## Physics-Motivated Design

### Lorentz Symmetry

Special relativity requires that physics is **invariant** under:
- **Boosts**: Change of reference frame (moving observer)
- **Rotations**: Change of spatial orientation

HyperVAE enforces this via L-GATr:
```
If input jets transform as: p â†’ Î›p (Lorentz boost Î›)
Then outputs transform as: p' â†’ Î›p' (same boost)
```

This is not just a nice propertyâ€”it's a **physical requirement** for realistic jet generation.

### Energy-Momentum Conservation

Jets must conserve 4-momentum:
```
Î£_i E_i = E_jet
Î£_i pâƒ—_i = pâƒ—_jet
```

HyperVAE enforces this via:
1. **Jet feature loss**: Predicts `[jet_pt, jet_eta, jet_mass]` from particles
2. **Soft constraint**: Particles must sum to correct jet 4-momentum
3. **Training signal**: Provides gradient for particle generation

### Permutation Invariance

Jet particle ordering is arbitrary (no physical meaning). HyperVAE uses:
- **Chamfer distance**: Permutation-invariant set loss
- **No fixed ordering**: Model doesn't depend on particle sequence
- **Symmetric architecture**: Encoder/decoder treat particles equally

---

## Implementation Highlights

### Training Strategies

1. **KL Annealing**: Gradually increase KL weight (0 â†’ 1) to prevent posterior collapse
2. **Temperature Annealing**: Gumbel-Softmax temperature (5.0 â†’ 0.5) for smooth topology learning
3. **Free Bits**: Regularize KL per dimension (3.0 bits) to preserve information
4. **Gradient Clipping**: Prevent explosion (max norm 3.0)
5. **Cosine Schedule**: Learning rate warmup + decay

### Loss Composition

```yaml
Total Loss = 12000 Ã— L_particle           # Chamfer distance (primary)
           + 1 Ã— L_edge_distribution      # 2-pt EEC Wasserstein
           + 1 Ã— L_hyperedge_distribution # N-pt EEC Wasserstein
           + 3000 Ã— L_jet                 # Jet features (constraint)
           + 3000 Ã— L_consistency         # Local-global physics
           + Î²(epoch) Ã— 0.3 Ã— L_KL        # KL divergence (annealed)
```

**Design principle**: Primary loss dominates (particles), distribution losses provide weak statistical guidance.

---

## Project Structure

```
hyperVAE/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hypervae.py          # Main VAE model
â”‚   â”œâ”€â”€ encoder.py           # Bipartite encoder
â”‚   â”œâ”€â”€ decoder.py           # Bipartite decoder
â”‚   â”œâ”€â”€ lgatr_wrapper.py     # L-GATr integration
â”‚   â””â”€â”€ lgat_layers.py       # Legacy attention layers
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bipartite_dataset.py # Dataset loader
â”‚   â””â”€â”€ data_utils.py        # Preprocessing utilities
â”œâ”€â”€ graph_constructor.py     # Graph dataset generation
â”œâ”€â”€ utils.py                 # Helper functions (EEC, normalization, hyperedges)
â”œâ”€â”€ train.py                 # Training script
â”œâ”€â”€ generate.py              # Jet generation script
â”œâ”€â”€ evaluate.py              # Evaluation metrics
â”œâ”€â”€ validate_data.py         # Data format validation
â”œâ”€â”€ config.yaml              # Model configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ setup.sh                 # Automated setup script
â”œâ”€â”€ MODEL_ARCHITECTURE.md    # Detailed technical docs
â””â”€â”€ PROJECT_OVERVIEW.md      # This file
```

---

## Data Preprocessing

Before training, jets must be converted to PyG graph format with `graph_constructor.py`:

### Graph Construction Pipeline

```
JetNet/EnergyFlow Dataset
    â†“
Feature Engineering: (pt, Î·, Ï†) â†’ (E, px, py, pz)
    â†“
Global Normalization: z-score or min-max (with statistics storage)
    â†“
EEC Computation: 2-point & n-point Energy-Energy Correlators
    â†“
Graph Building:
  â€¢ Nodes: Particle 4-momenta
  â€¢ Edges: 2pt-EEC + IRC-safe features (ln_delta, ln_kT, ln_z, ln_mÂ²)
  â€¢ Hyperedges: N-point particle combinations with n-point EEC
    â†“
PyG Data Objects â†’ Saved as .pt files
```

### Key Features:
- **Configurable normalization** (zscore/minmax) with reversibility
- **Memory-optimized** batch saving (85k jets per file)
- **Physics observables**: Energy-Energy Correlators for jet substructure
- **Complete statistics storage** for denormalization during generation

### Usage:

```bash
# Generate graph dataset (edit GRAPH_CONSTRUCTION_CONFIG in file)
python graph_constructor.py

# Default: 18k jets from JetNet (q, g, t types)
# Output: data/real/graphs_pyg_particle__fully_connected_*.pt
```

**For preprocessing details**, see module docstring in `graph_constructor.py`.

---

## ğŸš¦ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/fijicist/hyperVAE.git
cd hyperVAE

# Run automated setup
bash setup.sh
```

The setup script automatically:
- Detects CUDA version and GPU
- Installs PyTorch with correct CUDA support
- Installs PyG (PyTorch Geometric) with matching wheels
- Installs L-GATr and physics libraries
- Verifies all dependencies
- Runs quickstart test

### 2. Generate Graph Dataset

```bash
# Generate PyG graphs from JetNet dataset
python graph_constructor.py

# Output: data/real/graphs_pyg_particle__fully_connected_*.pt
```

**Optional**: Edit `GRAPH_CONSTRUCTION_CONFIG` in the file to customize:
- Dataset size (`N`)
- Normalization method (`zscore` or `minmax`)
- EEC orders and binning
- Output directory

### 3. Validate Your Data

```bash
python validate_data.py data/real/graphs_pyg_particle__fully_connected_final.pt
```

Expected format: List of PyG `Data` objects with:
- `x`: [N, 4] - Normalized 4-momenta `[E, px, py, pz]`
- `edge_index`: [2, M] - Fully-connected topology
- `edge_attr`: [M, 5] - Edge features (2pt_EEC, ln_delta, ln_kT, ln_z, ln_mÂ²)
- `hyperedge_index`: [N, K] - Hyperedge incidence matrix (optional)
- `hyperedge_attr`: [K, F] - N-point EEC features (optional)
- `y`: [4] - Jet labels `[type, log(pt), eta, log(mass)]`
- `particle_norm_stats`, `jet_norm_stats`, `edge_norm_stats`, `hyperedge_norm_stats` - For denormalization

### 4. Train Model

```bash
python train.py \
    --config config.yaml \
    --data-path data/real/graphs_pyg_particle__fully_connected_final.pt \
    --save-dir checkpoints \
    --log-dir runs
```

Monitor training:
```bash
tensorboard --logdir runs
```

### 5. Generate Jets

```bash
python generate.py \
    --checkpoint checkpoints/best_model.pt \
    --output generated_jets.pt \
    --num-samples 10000 \
    --gpu
```

Output: PyG `Data` objects ready for analysis.

---
## ğŸ“ˆ Use Cases

### 1. **Fast Simulation**

Replace slow Monte Carlo generators:
- **PYTHIA**: ~1 second/event
- **HyperVAE**: ~1 millisecond/event (1000Ã— speedup)

**Application**: Generate millions of jets for detector studies, systematic uncertainty estimation.

### 2. **Data Augmentation**

Increase training data for ML classifiers:
- Train on limited real data
- Augment with HyperVAE-generated jets
- Improve classifier performance

**Application**: Rare signal searches (e.g., SUSY, dark matter).

### 3. **Physics Studies**

Explore parameter space:
- Interpolate between quark/gluon jets
- Study jet substructure variations
- Generate jets with modified physics

**Application**: Understand systematic uncertainties, optimize detector design.

### 4. **Anomaly Detection**

Identify unusual jets:
- Train VAE on Standard Model jets
- Flag high-reconstruction-error jets
- Potential new physics signals

**Application**: Model-independent searches for beyond-Standard-Model phenomena.

---

## Technical Innovations

1. **First application of L-GATr** (Lorentz Group Attention) to jet generation
2. **Squared distance Chamfer loss** for stable gradient flow
3. **Bipartite hypergraph** representation capturing higher-order correlations
4. **Memory-optimized** for consumer-grade GPUs (4GB VRAM)
5. **Distribution matching** using Wasserstein distance for graph observables

---

## Documentation

- **[MODEL_ARCHITECTURE.md](MODEL_ARCHITECTURE.md)**: Comprehensive technical documentation
  - Encoder/decoder architecture
  - Loss functions with mathematical formulations
  - Training strategies (annealing, regularization)
  - Configuration parameters
  - Design decisions and innovations

- **Code Documentation**: All modules extensively commented
  - `models/hypervae.py`: ~400 lines of docstrings
  - `models/lgatr_wrapper.py`: ~150 lines explaining L-GATr
  - `train.py`: Training loop documentation
  - `generate.py`: Generation pipeline
  - `data/bipartite_dataset.py`: Data format and preprocessing

---

## ğŸ”¬ Physics Background

### Jet Physics Primer

**Jets** form when high-energy quarks/gluons fragment into hadrons:

```
Quark/Gluon (invisible) â†’ Parton Shower (QCD radiation)
                        â†’ Hadronization (confinement)
                        â†’ Jet (observable particles)
```

**Key properties**:
- **Transverse momentum (pT)**: Energy perpendicular to beam (100-1000 GeV)
- **Pseudorapidity (Î·)**: Angular coordinate along beam (-2.5 to 2.5)
- **Mass (m)**: Invariant mass of jet (0-200 GeV for light jets)
- **Constituents**: Number of particles (10-50 typical)

**Jet types**:
- **Quark jets**: Narrow, fewer particles, lower mass
- **Gluon jets**: Wider, more particles, higher mass
- **Top jets**: Boosted top quark decay (complex substructure)

### Why VAEs?

VAEs are ideal for jet generation because:

1. **Latent space**: Compresses high-dimensional jets â†’ low-dimensional codes
2. **Probabilistic**: Captures jet distribution uncertainty
3. **Interpolation**: Smooth transitions in latent space â†’ physically meaningful jets
4. **Conditioning**: Control jet type (quark/gluon/top) via latent code

### Why Lorentz Equivariance?

Special relativity is fundamental:
- Jets are produced in different reference frames
- Detector measures jets from lab frame
- Simulations must be frame-independent

**L-GATr ensures**: Generated jets transform correctly under Lorentz boosts â†’ physically consistent across all reference frames.

---

## Roadmap

### Current Status
- Core VAE architecture implemented
- L-GATr integration for Lorentz equivariance
- Squared distance Chamfer loss
- Distribution matching with Wasserstein distance
- Memory optimization (4GB VRAM)
- Comprehensive documentation

### Future Work
- [ ] Larger-scale training (100k+ jets)
- [ ] Evaluation metrics (Wasserstein distance, FID, physics observables)
- [ ] Comparison with PYTHIA/HERWIG generators
- [ ] Conditional generation (specify jet pT, Î·, mass)
- [ ] Uncertainty quantification
- [ ] Integration with detector simulation (GEANT4)

---

## ğŸ“„ Citation

If you use HyperVAE in your research, please cite:

```bibtex
@software{hypervae2025,
  title = {HyperVAE: Bipartite Hypergraph Variational Autoencoder for Jet Generation},
  author = {[Author Names]},
  year = {2025},
  url = {https://github.com/fijicist/hyperVAE}
}
```
