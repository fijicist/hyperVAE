"""
═══════════════════════════════════════════════════════════════════════════════
DECODER - VAE LATENT SPACE TO PARTICLE JET
═══════════════════════════════════════════════════════════════════════════════

Decodes VAE latent vectors into particle jets (4-momenta representation).

Architecture:
-------------
    Latent z [latent_dim] + Jet Type [3] → MLP Expander
                         ↓
    ┌────────────────────┴───────────────────┐
    │                                         │
    Topology Decoder                    Feature Decoders
    (Gumbel-Softmax)                    (L-GATr + MLPs)
    │                                         │
    └─ Particle mask [max_particles]         ├─ Particles → L-GATr → [N, 4]
                                              └─ Jet Features → MLP → [3]
                         ↓
                 Output: Particles + Jet Features
            (4-momenta: E, px, py, pz + jet_pt, jet_eta, jet_mass)

Key Components:
---------------
1. Latent Expander: Projects latent z to hidden states
2. Topology Decoder: Predicts particle existence (Gumbel-Softmax)
3. L-GATr Particle Decoder: Generates physically-valid 4-momenta
4. Jet Feature Head: Predicts jet-level observables (pt, eta, mass)

Note: Edge/hyperedge features computed from particles during loss calculation
      (not generated by decoder - see distribution loss in hypervae.py)

Generation Modes:
-----------------
- Training: Gumbel-Softmax with temperature annealing (differentiable sampling)
- Inference: Hard thresholding at 0.5 (deterministic for reproducibility)

Outputs:
--------
Dictionary with:
    'particle_features': [batch, max_particles, 4] - 4-momenta (E, px, py, pz)
    'particle_mask': [batch, max_particles] - Validity mask
    'topology': dict with n_particles prediction
    'jet_features': [batch, 3] - (jet_pt, jet_eta, jet_mass)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from .lgatr_wrapper import LGATrParticleDecoder
from torch_geometric.nn import GATv2Conv
import math


class BipartiteDecoder(nn.Module):
    """
    Decoder for particle jet VAE.
    
    Generates particle 4-momenta and jet features from latent code.
    
    Architecture:
    1. Latent z + Jet Type → MLP Expander
    2. Topology Decoder: Particle existence mask (Gumbel-Softmax)
    3. Feature Decoders:
       - Particles: L-GATr → (E, px, py, pz)
       - Jet: MLP → (jet_pt, jet_eta, jet_mass)
    
    Note: Edge/hyperedge observables computed from particles during training.
    """
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # Dimensions
        latent_dim = config['latent_dim']
        max_particles = config['max_particles']
        dropout = config['decoder']['dropout']
        jet_types = 3
        
        # Get L-GATr output dimension (scalars)
        lgatr_config = config.get('decoder', {}).get('lgatr', {})
        lgatr_scalar_dim = lgatr_config.get('in_s_channels', 64)  # Scalars from latent
        
        # 1. MLP Expander with residual path (latent + jet type -> hidden states)
        # PARTICLE-CENTRIC: Only expand to lgatr_scalar_dim (for particle conditioning)
        # Edge/hyperedge features will be recomputed from particles in HyperVAE
        output_dim = lgatr_scalar_dim
        
        # Pre-projection for residual
        self.latent_pre_proj = nn.Linear(latent_dim + jet_types, output_dim)
        
        # Main expansion path - DEEPER for better expressivity
        # 3 hidden layers with increasing then decreasing width
        self.latent_expander = nn.Sequential(
            nn.Linear(latent_dim + jet_types, latent_dim * 3),
            nn.LayerNorm(latent_dim * 3),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(latent_dim * 3, latent_dim * 2),
            nn.LayerNorm(latent_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(latent_dim * 2, latent_dim),
            nn.LayerNorm(latent_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(latent_dim, output_dim)
        )
        
        # 2. Topology Decoder (PARTICLE-CENTRIC: only predicts particle existence)
        # Edge/hyperedge topology will be recomputed from particles in HyperVAE
        self.topology_decoder = TopologyDecoder(
            lgatr_scalar_dim, max_particles, config['decoder']['use_gumbel_softmax']
        )
        
        # 3. Particle Feature Decoder using official L-GATr
        self.particle_decoder = LGATrParticleDecoder(config)
        
        # 3.5. Per-particle variation module (for particle diversity)
        # Creates unique features per particle using particle index + latent
        # Prevents all particles from having identical initial states
        # DEEPER architecture for better expressivity
        self.particle_variation = nn.Sequential(
            nn.Linear(lgatr_scalar_dim + 1, lgatr_scalar_dim * 2),  # +1 for particle index
            nn.LayerNorm(lgatr_scalar_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(lgatr_scalar_dim * 2, lgatr_scalar_dim),
            nn.LayerNorm(lgatr_scalar_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(lgatr_scalar_dim, lgatr_scalar_dim)
        )
        
        # Learnable scale parameter for per-particle variation
        # Controls how much variation is applied (model learns optimal value during training)
        # Initial value from config (default: 0.5, range typically 0.1-1.0)
        initial_scale = config['decoder'].get('particle_variation_scale', 0.5)
        self.particle_variation_scale = nn.Parameter(torch.tensor(initial_scale))
        
        # 3.6. Initial 4-momentum projection from per-particle scalars
        # Projects unique per-particle scalars to initial 4-momentum guess
        # This gives L-GATr meaningful, diverse starting points for refinement
        # DEEPER architecture with intermediate hidden layers for better expressivity
        self.scalar_to_4mom_hidden = nn.Sequential(
            nn.Linear(lgatr_scalar_dim, lgatr_scalar_dim * 2),
            nn.LayerNorm(lgatr_scalar_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(lgatr_scalar_dim * 2, lgatr_scalar_dim),
            nn.LayerNorm(lgatr_scalar_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        self.scalar_to_4mom_out = nn.Linear(lgatr_scalar_dim, 4)
        
        # Pre-compute and cache particle indices for efficiency (computed once, reused)
        # Normalized indices [0, 1/(max_p-1), 2/(max_p-1), ..., 1.0]
        # Leading particles (low index) vs soft particles (high index)
        self.register_buffer(
            'particle_indices_normalized',
            torch.arange(max_particles, dtype=torch.float32) / max(max_particles - 1, 1)
        )
        
        # 4. Jet Feature Decoder (configurable depth and width)
        jet_layers_count = config['decoder'].get('jet_layers', 3)  
        jet_hidden_dim = config.get('jet_hidden', latent_dim)  
        jet_dropout = config['decoder'].get('dropout', 0.2)
        feature_proj_dropout = config['decoder'].get('feature_proj_dropout', 0.2)
        
        # Build jet feature decoder with configurable depth
        jet_layers = []
        input_dim = latent_dim + jet_types
        
        # Input projection
        jet_layers.extend([
            nn.Linear(input_dim, jet_hidden_dim),
            nn.LayerNorm(jet_hidden_dim),
            nn.GELU(),
            nn.Dropout(jet_dropout)
        ])
        
        # Hidden layers with residual connections
        for _ in range(jet_layers_count - 1):
            jet_layers.extend([
                nn.Linear(jet_hidden_dim, jet_hidden_dim),
                nn.LayerNorm(jet_hidden_dim),
                nn.GELU(),
                nn.Dropout(jet_dropout)
            ])
        
        self.jet_mlp = nn.Sequential(*jet_layers)
        
        # Deeper feature projectors for better expressivity (4-layer MLPs)
        # One projector per jet feature: jet_pt, jet_eta, jet_mass
        num_jet_features = config.get('jet_features', 3)
        self.jet_feature_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(jet_hidden_dim, jet_hidden_dim),
                nn.LayerNorm(jet_hidden_dim),
                nn.GELU(),
                nn.Dropout(feature_proj_dropout),
                nn.Linear(jet_hidden_dim, jet_hidden_dim),
                nn.LayerNorm(jet_hidden_dim),
                nn.GELU(),
                nn.Dropout(feature_proj_dropout),
                nn.Linear(jet_hidden_dim, jet_hidden_dim // 2),
                nn.GELU(),
                nn.Dropout(feature_proj_dropout),
                nn.Linear(jet_hidden_dim // 2, 1)
            )
            for _ in range(num_jet_features)
        ])
        
        self.max_particles = max_particles
    
    @staticmethod
    def _calculate_combinations(n, k):
        """Calculate C(n, k) = n! / (k! * (n-k)!) efficiently."""
        if k > n or k < 0:
            return 0
        if k == 0 or k == n:
            return 1
        # Use the more efficient formula: product from 1 to k
        result = 1
        for i in range(min(k, n - k)):
            result = result * (n - i) // (i + 1)
        return result
    
    def forward(self, z, jet_type, temperature=1.0):
        """
        Args:
            z: Latent vectors [batch_size, latent_dim]
            jet_type: Jet types [batch_size]
            temperature: Gumbel-softmax temperature
        
        Returns:
            Dictionary with generated features and topology
        """
        batch_size = z.size(0)
        device = z.device
        
        # One-hot encode jet type
        jet_type_onehot = F.one_hot(jet_type, num_classes=3).float()
        latent_input = torch.cat([z, jet_type_onehot], dim=-1)
        
        # 1. Expand latent with residual connection
        latent_proj = self.latent_pre_proj(latent_input)  # Direct projection
        latent_expanded = latent_proj + self.latent_expander(latent_input)  # Residual
        
        # PARTICLE-CENTRIC: latent_expanded is now just particle_scalars (no edge/hyperedge chunks)
        lgatr_config = self.config.get('decoder', {}).get('lgatr', {})
        lgatr_scalar_dim = lgatr_config.get('in_s_channels', 64)
        
        particle_scalars = latent_expanded  # [batch_size, lgatr_scalar_dim]
        
        # 2. Decode topology (PARTICLE-ONLY)
        topology = self.topology_decoder(particle_scalars, temperature)
        
        # 3. ALWAYS decode particle features using official L-GATr
        batch_size_actual = z.size(0)
        max_p = topology['particle_mask'].size(1)
        device = z.device
        
        # Generate per-particle scalars with positional encoding
        # Get cached indices (pre-computed in __init__) [max_p]
        indices = self.particle_indices_normalized[:max_p].view(1, max_p, 1)
        
        # Expand base scalars [batch, max_p, lgatr_scalar_dim]
        particle_scalars_base = particle_scalars.unsqueeze(1).expand(batch_size_actual, max_p, -1)
        
        # Single concat operation [batch, max_p, lgatr_scalar_dim+1]
        particle_scalars_with_idx = torch.cat([
            particle_scalars_base, 
            indices.expand(batch_size_actual, -1, -1)
        ], dim=-1)
        
        # Single reshape + forward + fused scale application
        # Flatten [batch*max_p, lgatr_scalar_dim+1]
        flat_input = particle_scalars_with_idx.view(-1, lgatr_scalar_dim + 1)
        
        # Generate variation [batch*max_p, lgatr_scalar_dim]
        particle_variation_flat = self.particle_variation(flat_input)
        
        # Fused operations - add base + scaled variation in one step
        # Flatten base for addition
        particle_scalars_base_flat = particle_scalars_base.reshape(-1, lgatr_scalar_dim)
        
        # Apply scale and combine (fused operation, more efficient)
        scale = torch.sigmoid(self.particle_variation_scale)
        particle_scalars_expanded = (
            particle_scalars_base_flat + particle_variation_flat * scale
        ).view(batch_size_actual, max_p, -1)
        
        # Generate per-particle initial 4-momenta [batch, max_p, 4]
        # Each particle gets unique initial guess based on its position and latent
        # Use deeper projection with residual connection for better expressivity
        scalars_flat = particle_scalars_expanded.view(-1, lgatr_scalar_dim)
        hidden_4mom = self.scalar_to_4mom_hidden(scalars_flat)
        # Residual connection before final projection
        hidden_4mom = hidden_4mom + scalars_flat
        initial_fourmomenta = self.scalar_to_4mom_out(hidden_4mom).view(batch_size_actual, max_p, 4)
        
        # Add small noise for regularization (stochastic gradient)
        initial_fourmomenta = initial_fourmomenta + torch.randn_like(initial_fourmomenta) * 0.01
        
        # Pass through L-GATr decoder
        lgatr_output = self.particle_decoder(
            fourmomenta=initial_fourmomenta,
            scalars=particle_scalars_expanded,  # Condition on latent scalars
            mask=topology['particle_mask']
        )
        
        # Extract 4-vectors [E, px, py, pz]
        particle_features = lgatr_output['fourmomenta']  # [batch_size, max_particles, 4]
        
        # 4. Edge/hyperedge generation REMOVED (PARTICLE-CENTRIC)
        # Edge and hyperedge features will be recomputed from particles in HyperVAE
        # This simplifies the decoder and ensures consistency
        
        # 5. Decode jet-level features (ALWAYS generate) with deep projectors
        # Process through MLP layers
        jet_h = self.jet_mlp(latent_input)  # [batch_size, jet_hidden_dim]
        
        # Generate each jet feature using deep projectors (similar to particles)
        jet_feature_list = []
        for projector in self.jet_feature_projectors:
            feature = projector(jet_h).squeeze(-1)  # [batch_size]
            jet_feature_list.append(feature)
        
        # Stack features [batch_size, num_jet_features (3)]
        jet_features = torch.stack(jet_feature_list, dim=-1)
        
        # PARTICLE-CENTRIC RETURN: Only particles, particle_mask, topology, and jet_features
        return {
            'particle_features': particle_features,  # [batch_size, max_particles, 4]
            'particle_mask': topology['particle_mask'],  # [batch_size, max_particles]
            'topology': {
                'n_particles': topology['n_particles'],  # [batch_size]
                'particle_logits': topology['particle_logits']  # [batch_size, max_particles]
            },
            'jet_features': jet_features  # [batch_size, 3] - jet_pt, jet_eta, jet_mass
        }


class TopologyDecoder(nn.Module):
    """Decode particle topology with Gumbel-Softmax (PARTICLE-CENTRIC)"""
    
    def __init__(self, particle_hidden, max_particles, use_gumbel_softmax=True):
        super().__init__()
        self.max_particles = max_particles
        self.use_gumbel_softmax = use_gumbel_softmax
        
        # Particle existence predictor
        self.particle_exist_mlp = nn.Sequential(
            nn.Linear(particle_hidden, particle_hidden),
            nn.GELU(),
            nn.Linear(particle_hidden, max_particles)
        )
    
    def forward(self, particle_h, temperature=1.0):
        """Generate particle topology only"""
        batch_size = particle_h.size(0)
        device = particle_h.device
        
        # Particle existence
        particle_logits = self.particle_exist_mlp(particle_h)
        if self.use_gumbel_softmax and self.training:
            particle_mask = F.gumbel_softmax(
                torch.stack([particle_logits, -particle_logits], dim=-1),
                tau=temperature, hard=True
            )[..., 0]
        else:
            particle_mask = (torch.sigmoid(particle_logits) > 0.5).float()
        
        n_particles = particle_mask.sum(dim=-1).long()
        
        return {
            'particle_mask': particle_mask,
            'particle_logits': particle_logits,
            'n_particles': n_particles
        }


if __name__ == "__main__":
    # Test decoder
    config = {
        'latent_dim': 128,
        'particle_hidden': 64,
        'max_particles': 150,
        'decoder': {
            'mlp_layers': 3,
            'dropout': 0.1,
            'use_gumbel_softmax': True,
            'gumbel_temperature': 1.0
        }
    }
    
    decoder = BipartiteDecoder(config)
    
    z = torch.randn(4, 128)
    jet_type = torch.tensor([0, 1, 2, 0])
    
    output = decoder(z, jet_type)
    
    print(f"Particle features: {output['particle_features'].shape}")
    print(f"Particle mask: {output['particle_mask'].shape}")
    print(f"Jet features: {output['jet_features'].shape}")
