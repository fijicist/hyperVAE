# HyperVAE: Lorentz-Equivariant Hypergraph VAE for Jet Generation

**HyperVAE** is a physics-informed deep generative model for synthesizing high-energy physics jets using Variational Autoencoders with Lorentz-equivariant transformations. The model represents jets as bipartite hypergraphs with particle nodes, pairwise edges encoding 2-point Energy-Energy Correlators (EEC), and hyperedges capturing higher-order N-point correlations. By leveraging L-GATr (Lorentz Group Attention) layers, HyperVAE aims to generate jets that respect special relativity and fundamental spacetime symmetries.

Designed for **consumer-grade GPUs (4GB VRAM)**, HyperVAE employs memory-optimized training strategies including gradient accumulation, mixed-precision computation, and efficient PyG batching. Deep generative models offer the potential for significantly faster jet generation compared to traditional physics simulators, which could be useful for data augmentation, detector studies, and physics analyses.

---

## üèóÔ∏è Model Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            INPUT: Real Jet Data                             ‚îÇ
‚îÇ                  PyG Graphs: Particles + Edges + Hyperedges                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
                                     ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ              ENCODER (q_œÜ)                        ‚îÇ
         ‚îÇ                                                   ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
         ‚îÇ  ‚îÇ   Particle Encoding (L-GATr)            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ Lorentz-equivariant attention       ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ Input: [E, px, py, pz]              ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ Output: Scalar features             ‚îÇ     ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
         ‚îÇ                    ‚îÇ                             ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
         ‚îÇ  ‚îÇ   Edge/Hyperedge Encoding (MLPs)        ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ 2-point EEC features                ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ N-point EEC features                ‚îÇ     ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
         ‚îÇ                    ‚îÇ                             ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
         ‚îÇ  ‚îÇ   Cross-Attention Fusion                ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ Particle ‚Üî Edge interactions        ‚îÇ     ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
         ‚îÇ                    ‚îÇ                             ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
         ‚îÇ  ‚îÇ   Global Pooling (mean/max)             ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ Aggregates to jet-level embedding   ‚îÇ     ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
         ‚îÇ                    ‚îÇ                             ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
         ‚îÇ  ‚îÇ   Latent Projection                     ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Üí Œº(x), log(œÉ¬≤(x))                    ‚îÇ     ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  LATENT SPACE (z)     ‚îÇ
                    ‚îÇ  z ~ N(Œº, œÉ¬≤)         ‚îÇ
                    ‚îÇ  [Reparameterization] ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ              DECODER (p_Œ∏)                        ‚îÇ
         ‚îÇ                                                   ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
         ‚îÇ  ‚îÇ   Latent Expansion + Conditioning       ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ Broadcast z to N particles          ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ Concat jet type embedding           ‚îÇ     ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
         ‚îÇ                    ‚îÇ                             ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
         ‚îÇ  ‚îÇ   Topology Decoder (Gumbel-Softmax)     ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ Predict particle multiplicity       ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚Ä¢ Differentiable sampling             ‚îÇ     ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
         ‚îÇ                    ‚îÇ                             ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
         ‚îÇ  ‚îÇ   Parallel Feature Decoders:            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ                 ‚îÇ                       ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îÇ Particle Decoder       ‚îÇ            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îÇ (L-GATr)               ‚îÇ            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îÇ ‚Üí [E, px, py, pz]      ‚îÇ            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ                                         ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îÇ Edge Decoder (MLP)     ‚îÇ            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îÇ ‚Üí 2pt-EEC + features   ‚îÇ            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ                                         ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îÇ Hyperedge Decoder (MLP)‚îÇ            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îÇ ‚Üí Npt-EEC features     ‚îÇ            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ                                         ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îÇ Jet Feature Head       ‚îÇ            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îÇ ‚Üí [pt, eta, mass]      ‚îÇ            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ   LOSS COMPUTATION             ‚îÇ
                ‚îÇ                                ‚îÇ
                ‚îÇ  ‚Ä¢ Chamfer Distance (particles)‚îÇ
                ‚îÇ  ‚Ä¢ MSE (edges, hyperedges)     ‚îÇ
                ‚îÇ  ‚Ä¢ Jet Feature Loss            ‚îÇ
                ‚îÇ  ‚Ä¢ KL Divergence (annealed)    ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Components:**
- **L-GATr Layers**: Ensure Lorentz equivariance (boosts, rotations)
- **Bipartite Structure**: Particles + Edges + Hyperedges
- **Gumbel-Softmax**: Differentiable topology learning
- **Multi-Task Learning**: Particles (primary) + Auxiliary features

---

## üöÄ Quick Start

### Prerequisites
- **Python**: 3.8+ (3.10 recommended)
- **GPU**: CUDA-capable (optional, but recommended)
- **Memory**: 4GB VRAM minimum (GTX 1650 Ti or better)

### 1. Installation

```bash
# Clone repository
git clone https://github.com/fijicist/hyperVAE.git
cd hyperVAE

# Automated setup (recommended)
bash setup.sh
```

The setup script will:
- ‚úÖ Auto-detect your Python version and CUDA version
- ‚úÖ Install PyTorch with correct CUDA support (or CPU-only)
- ‚úÖ Install PyTorch Geometric with matching wheels
- ‚úÖ Install L-GATr, FastJet, and physics libraries
- ‚úÖ Verify all dependencies
- ‚úÖ Run a quickstart test

**Manual installation** (if setup.sh fails):
```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or
.venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

---

### 2. Generate Graph Dataset

Before training, preprocess raw jet data into PyG graph format:

```bash
# Generate graphs from JetNet dataset
python graph_constructor.py
```

**What this does:**
- Loads jets from JetNet (quark, gluon, top jets)
- Transforms to 4-momentum: `(pt, Œ∑, œÜ) ‚Üí (E, px, py, pz)`
- Computes Energy-Energy Correlators (2-point, 3-point, ...)
- Builds fully-connected graphs with edge/hyperedge features
- Applies global normalization (z-score or min-max)
- Saves to `data/real/graphs_pyg_particle__fully_connected_*.pt`

**Configuration**: Edit `GRAPH_CONSTRUCTION_CONFIG` in the file to customize:
```python
{
    'N': 18000,                    # Number of jets
    'normalization_method': 'zscore',  # 'zscore' or 'minmax'
    'eec_prop': [[2, 3], 200, (1e-4, 2)],  # EEC orders and binning
    'output_dir': './data/real/',
}
```

**Expected output:**
```
data/real/
‚îú‚îÄ‚îÄ graphs_pyg_particle__fully_connected_part_1.pt  (if > 85k jets)
‚îî‚îÄ‚îÄ graphs_pyg_particle__fully_connected_final.pt
```

---

### 3. Validate Data Format

Ensure generated graphs have correct structure:

```bash
python validate_data.py data/real/graphs_pyg_particle__fully_connected_final.pt
```

**Expected format:**
```python
Data(
    x=[N, 4],              # Normalized particle 4-momenta [E, px, py, pz]
    edge_index=[2, M],     # Fully-connected topology
    edge_attr=[M, 5],      # [2pt_EEC, ln_delta, ln_kT, ln_z, ln_m¬≤]
    hyperedge_index=[N, K], # N-point incidence matrix (optional)
    hyperedge_attr=[K, F], # N-point EEC features (optional)
    y=[4],                 # [jet_type, log(pt), eta, log(mass)]
    particle_norm_stats={}, # For denormalization
    jet_norm_stats={},
    edge_norm_stats={},
    hyperedge_norm_stats={},
)
```

---

### 4. Train Model

Start training with default configuration:

```bash
python train.py \
    --config config.yaml \
    --data-path data/real/graphs_pyg_particle__fully_connected_final.pt \
    --save-dir checkpoints \
    --log-dir runs
```

**Training configuration** (`config.yaml`):
```yaml
model:
  latent_dim: 256           # Latent space size
  particle_hidden_dim: 512  # L-GATr hidden dimensions
  
training:
  batch_size: 2             # Per-GPU batch size
  gradient_accumulation_steps: 128  # Effective batch = 2√ó128 = 256
  num_epochs: 300
  learning_rate: 0.0001
  mixed_precision: true     # Enable AMP for memory efficiency
  precision_type: "fp16"    # "fp16" (Volta/Turing) or "bf16" (Ampere+)
  
loss_weights:
  particle: 12000.0         # Primary loss
  edge: 250.0               # Auxiliary
  hyperedge: 150.0          # Auxiliary
  jet_features: 6000.0      # Soft constraint
  kl_weight: 0.3            # Annealed during training
```

**Precision type selection:**
- **BF16** (`precision_type: "bf16"`): Ampere+ GPUs (RTX 30xx/40xx, A100)
  - Better numerical stability, wider dynamic range
  - Recommended for newer GPUs
- **FP16** (`precision_type: "fp16"`): Volta/Turing GPUs (V100, T4, RTX 20xx)
  - 2√ó memory savings, good for older architectures

**Check your GPU compatibility:**
```bash
python test_bf16.py  # Shows GPU compute capability and BF16 support
```

**Monitor training:**
```bash
# In another terminal
tensorboard --logdir runs
```

Visit `http://localhost:6006` to see:
- Loss curves (particle, edge, KL, total)
- Learning rate schedule
- Gradient norms

**Expected training time:**
- **GTX 1650 Ti (4GB)**: ~12 hours for 300 epochs (18k jets)
- **RTX 3090 (24GB)**: ~3 hours

**Note**: Training time varies based on hardware and dataset size. The model is designed to be trainable on consumer hardware, though performance may vary.

**Checkpoints saved:**
- `checkpoints/best_model.pt` - Lowest validation loss
- `checkpoints/checkpoint_epoch_*.pt` - Regular snapshots

---

### 5. Generate Jets

Sample new jets from trained model:

```bash
python generate.py \
    --checkpoint checkpoints/best_model.pt \
    --output generated_jets.pt \
    --num-samples 10000 \
    --jet-type q \
    --gpu
```

**Options:**
- `--num-samples`: Number of jets to generate
- `--jet-type`: `q` (quark), `g` (gluon), `t` (top), or `None` (sample from prior)
- `--temperature`: Sampling temperature (default: 1.0, lower = more conservative)
- `--batch-size`: Generation batch size (default: 256)

**Output format:**
```python
# List of PyG Data objects
[
    Data(x=[N‚ÇÅ, 4], edge_attr=[M‚ÇÅ, 5], y=[4]),
    Data(x=[N‚ÇÇ, 4], edge_attr=[M‚ÇÇ, 5], y=[4]),
    ...
]
```

**Denormalization** happens automatically using stored `*_norm_stats` from training data.

---

### 6. Evaluate Generated Jets

Compare generated jets to real data:

```bash
python evaluate.py \
    --real-data data/real/graphs_pyg_particle__fully_connected_final.pt \
    --generated-data generated_jets.pt \
    --output-dir evaluation_results
```

**Metrics computed:**
- **Particle-level**: Multiplicity, pT, Œ∑, œÜ distributions
- **Jet-level**: Mass, pT, Œ∑ distributions
- **Physics observables**: 
  - Energy-Energy Correlators (EEC)
  - N-subjettiness ratios
  - Jet mass, jet width
- **Statistical comparisons**: Distribution overlaps and Wasserstein distances

**Note**: Evaluation metrics are being developed and validated. Physics fidelity assessment is an ongoing area of research.

**Output:**
```
evaluation_results/
‚îú‚îÄ‚îÄ particle_distributions.png
‚îú‚îÄ‚îÄ jet_distributions.png
‚îú‚îÄ‚îÄ eec_comparison.png
‚îî‚îÄ‚îÄ metrics.json
```

## üîß Advanced Usage

### Resume Training from Checkpoint

```bash
python train.py \
    --config config.yaml \
    --data-path data/real/graphs_pyg_particle__fully_connected_final.pt \
    --resume checkpoints/checkpoint_epoch_100.pt \
    --save-dir checkpoints \
    --log-dir runs
```

### Multi-GPU Training

```bash
# Use DataParallel (simple)
python train.py --config config.yaml --data-path data/... --num-gpus 2

# Or use DistributedDataParallel (faster)
torchrun --nproc_per_node=2 train.py --config config.yaml --data-path data/...
```

### Custom Dataset

To use your own jet data:

1. **Format as PyG graphs**: See `graph_constructor.py` as template
2. **Required fields**: `x`, `edge_index`, `edge_attr`, `y`
3. **Validate**: `python validate_data.py your_data.pt`
4. **Train**: `python train.py --data-path your_data.pt`

---

## üìö Documentation

- **[PROJECT_OVERVIEW.md](PROJECT_OVERVIEW.md)**: High-level overview, physics motivation, architecture
- **[MODEL_ARCHITECTURE.md](MODEL_ARCHITECTURE.md)**: Detailed technical documentation
- **[requirements.txt](requirements.txt)**: Python dependencies with installation guide
- **Module docstrings**: Every Python file has comprehensive header documentation

---

## üêõ Troubleshooting

### Out of Memory (OOM)

```bash
# Reduce batch size in config.yaml
training:
  batch_size: 1
  gradient_accumulation_steps: 256
```

### CUDA Version Mismatch

```bash
# Reinstall PyTorch with correct CUDA
pip uninstall torch torch-geometric
bash setup.sh  # Auto-detects CUDA version
```

### Slow Training

- ‚úÖ Enable mixed precision: `mixed_precision: true` in config
- ‚úÖ Use BF16 on Ampere+ GPUs: `precision_type: "bf16"` (RTX 30xx/40xx, A100)
- ‚úÖ Use FP16 on Volta/Turing GPUs: `precision_type: "fp16"` (V100, T4, RTX 20xx)
- ‚úÖ Increase batch size if you have more VRAM
- ‚úÖ Use gradient checkpointing: `gradient_checkpointing: true`
- ‚úÖ Check GPU utilization: `nvidia-smi -l 1`

**Mixed Precision Guide:**
- **BF16 (bfloat16)**: Better numerical stability, wider dynamic range. Recommended for Ampere+ (SM 8.0+)
- **FP16 (float16)**: Faster on older GPUs, requires careful gradient scaling. Good for Volta/Turing
- Test your GPU: `python test_bf16.py`

### Poor Generation Quality

- ‚ö†Ô∏è Train longer (model may need 300+ epochs to converge)
- ‚ö†Ô∏è Check loss curves for plateaus or divergence
- ‚ö†Ô∏è Increase KL annealing warmup epochs for smoother training
- ‚ö†Ô∏è Verify data normalization statistics are computed correctly
- ‚ö†Ô∏è Consider adjusting loss weights if one component dominates

**Note**: Achieving high-quality jet generation is challenging and may require hyperparameter tuning and multiple training runs.

---

## üìù Citation

If you use HyperVAE in your research, please cite:

```bibtex
@software{hypervae2024,
  author = {Your Name},
  title = {HyperVAE: Lorentz-Equivariant Hypergraph VAE for Jet Generation},
  year = {2024},
  url = {https://github.com/fijicist/hyperVAE}
}
```

## üôè Acknowledgments

- **L-GATr**: [Brehmer et al., "Geometric Algebra Transformers"](https://arxiv.org/abs/2305.18415)
- **JetNet**: [Kansal et al., "JetNet Dataset"](https://arxiv.org/abs/2106.11535)
- **PyTorch Geometric**: [Fey & Lenssen](https://arxiv.org/abs/1903.02428)
- **EnergyFlow**: [Komiske et al.](https://energyflow.network/)

---
