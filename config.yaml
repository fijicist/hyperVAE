model:
  # Hidden dimensions 
  particle_hidden: 128        
  edge_hidden: 96             
  hyperedge_hidden: 64        
  jet_hidden: 96             
  latent_dim: 32    # Has to match the scalar channels of LGATr latent projection          
  max_particles: 60
  
  # Feature dimensions
  particle_features: 4        # [E, px, py, pz] (4-vector for L-GATr)
  edge_features: 5            # [2pt_EEC, ln_delta, ln_kT, ln_z, ln_m2]
  hyperedge_features: 1       # [3pt_EEC, and so on]
  jet_features: 3             # [jet_pt, jet_eta, jet_mass]
  
  # Architecture - Symmetric encoder/decoder 
  encoder:
    # Particle encoding (L-GATr layers with Lorentz equivariance)
    particle_layers: 3        # Number of L-GATr blocks (num_blocks parameter)
    
    # Cross-attention configuration (particle ↔ edge/hyperedge fusion)
    cross_attention_heads: 4  # Number of heads for bipartite cross-attention layers
    
    # ==========================================
    # L-GATr Configuration for Encoder
    # ==========================================
    # Based on official L-GATr library: https://heidelberg-hepml.github.io/lgatr/
    lgatr:
      # === CORE L-GATR PARAMETERS ===
      # Geometric Algebra (GA) Multivector Channels
      in_mv_channels: 1         # Input: 1 channel of 4-vectors (E, px, py, pz)
      out_mv_channels: 0        # Output: no multivector (project to scalars for latent)
      hidden_mv_channels: 16     # Hidden GA multivector channels
      
      # Scalar Channels (non-geometric features)
      in_s_channels: 0          # No additional scalar inputs (all info in 4-momentum)
      out_s_channels: 32        # Output: scalar embedding for latent projection
      hidden_s_channels: 32     # Hidden scalar channels
      
      # === TRANSFORMER ARCHITECTURE ===
      num_blocks: 3             # Number of L-GATr transformer blocks (overrides particle_layers if set)
      
      # Attention Configuration
      attention:
        num_heads: 4            # Multi-head attention (4 heads standard)
        multi_query: true       # Memory-efficient attention (critical for 4GB VRAM)
        dropout_prob: 0.25           # Attention dropout
        # use_flash_attention: false  # Flash attention (requires flash-attn package)
        head_scale: true     # Enable head scaling for stability
        
      # MLP Configuration (between attention layers)
      mlp:
        activation: "gelu"      # GELU activation (standard for transformers)
        num_hidden_layers: 2   # MLP depth in each block
    
    # Edge encoding (MLP layers with residual connections)
    edge_layers: 3            # Number of MLP layers for edges
    
    # Hyperedge encoding (MLP layers with residual connections)
    hyperedge_layers: 3       # Number of MLP layers for hyperedges
    
    # Jet-level feature encoding (MLP layers with residual connections)
    jet_layers: 3             # Number of MLP layers for jet features
    
    # Regularization
    dropout: 0.3             # Dropout rate for all encoder layers (non-LGATr)
    use_residual: true        # Enable residual connections (applies to all layers)
  
  decoder:
    # Particle decoding (L-GATr layers with Lorentz equivariance)
    particle_layers: 3        # Number of L-GATr blocks (matches encoder)
    
    # ==========================================
    # L-GATr Configuration for Decoder
    # ==========================================
    lgatr:
      # === CORE L-GATR PARAMETERS ===
      # Geometric Algebra (GA) Multivector Channels
      in_mv_channels: 1         # Input: 1 channel of 4-vectors (seed for generation)
      out_mv_channels: 1        # Output: 1 channel of 4-vectors (E, px, py, pz)
      hidden_mv_channels: 16     # Hidden GA multivector channels (match encoder)
      
      # Scalar Channels (non-geometric features)
      in_s_channels: 32         # Input: scalar features from latent
      out_s_channels: 0         # Output: no additional scalars (only 4-vectors)
      hidden_s_channels: 32     # Hidden scalar channels (match encoder)
      
      # === TRANSFORMER ARCHITECTURE ===
      num_blocks: 4             # Number of L-GATr transformer blocks (match encoder)
      
      # Attention Configuration
      attention:
        num_heads: 4            # Multi-head attention (match encoder)
        multi_query: true       # Memory-efficient attention
        dropout_prob: 0.25            # Attention dropout (match encoder)
        # use_flash_attention: false  # Flash attention
        head_scale: true     # Enable head scaling for stability
        
      # MLP Configuration (between attention layers)
      mlp:
        activation: "gelu"      # GELU activation (standard for transformers)
        num_hidden_layers: 2   # MLP depth in each block
    
    # Regularization
    dropout: 0.3              # Dropout rate for all decoder layers
    feature_proj_dropout: 0.3 # Dropout for feature projection layers
    
    # Per-particle variation scale (CRITICAL for diverse particle generation)
    # Controls how much each particle differs from others in a jet
    # Initial value for learnable parameter (model learns optimal value during training)
    # Range: 0.0 (no variation) to 1.0 (full variation)
    # Default: 0.5 (balanced - encourages diversity while allowing model to learn)
    # Set to 0.0 for ablation studies to test without per-particle variation
    particle_variation_scale: 0.5
    
    # Topology decoder settings
    use_gumbel_softmax: true
    # Note: Gumbel temperature is controlled by training.initial_temperature, 
    # training.final_temperature, and training.temperature_decay settings below

training:
  batch_size: 64                  # Effective batch = 64 * 4 = 256
  gradient_accumulation_steps: 8  # Good for memory constraints
  epochs: 200
  learning_rate: 0.00015           # Finer updates for stability
  weight_decay: 0.005             # Keep: L2 regularization
  latent_noise: 0.05              # Keep: noise for regularization
  mixed_precision: true           # Enable mixed precision training (FP16/BF16)
  precision_type: "fp16"          # Precision type: "fp16" (Volta+/Turing+) or "bf16" (Ampere+, better stability)
  
  # Gradient clipping (increased for squared distance metric)
  # Squared distance has linear gradients (∂d²/∂x = 2x) which are more stable than
  # Euclidean gradients (∂√d²/∂x = x/d) that can spike for small errors
  gradient_clip: 15.0
  
  # Scheduler
  scheduler: "cosine"
  warmup_epochs: 20
  min_lr: 0.000015
  
  # Loss weights for different components
  loss_weights:
    particle_features: 12000.0       # REDUCED from 12000 - slightly less dominant
    jet_features: 3000.0             # Jet-level feature loss (jet_pt, jet_eta, jet_mass)
    edge_distribution: 500.0         # INCREASED from 10 - 10x boost for EEC learning
    hyperedge_distribution: 400.0    # INCREASED from 10 - 10x boost for N-point EEC
    local_global_consistency: 400.0  # INCREASED from 5 - 20x boost for physics consistency
    kl_divergence: 1.0               # INCREASED from 0.3 - stronger regularization base
  
  # Loss function configuration
  loss_config:
    particle_loss_type: "hyperbolic_chamfer"  # Options: "chamfer", "hyperbolic_chamfer", "mse"
    particle_distance_metric: "euclidean_4d"  # Standard 4D Euclidean distance for [E, px, py, pz]
    
    # Particle feature indices [E, px, py, pz]
    E_index: 0                  # Index of energy E in particle features
    px_index: 1                 # Index of px momentum component
    py_index: 2                 # Index of py momentum component
    pz_index: 3                 # Index of pz momentum component
    
    # Squared distance vs Euclidean distance for usual Chamfer distance
    # Squared (d²): Strong gradients (∇d² = 2x), consistent training/val, faster computation
    # Euclidean (√d²): Standard metric but vanishing gradients for large errors (∇√d² = 1/(2√x))
    # Use squared for normalized features to avoid gradient plateau
    use_squared_distance: true   # Use squared distance for all epochs (training + validation)
    
    # Hyperbolic Chamfer distance configuration
    # Uses hyperbolic geometry transformation: d_hyp = arcosh(1 + α × d_E²)
    # Paper: arXiv:2412.17951 shows hyperbolic Chamfer better captures jet structure
    hyperbolic_alpha: 1.0        # Hyperparameter controlling hyperbolic curvature
                                 # α → 0: recovers standard Chamfer
                                 # α > 0: stronger hyperbolic effect, emphasizes large distances
    
    # pT-weighted Chamfer loss
    use_pt_weighting: false      # Enable pT-based weighting of Chamfer distance
    pt_weight_alpha: 0.5        # Exponent for pT weighting: α=1.0 (linear), α=2.0 (quadratic)
    
    evaluation_metric: "hyperbolic_chamfer"  # Use chamfer for validation too (was "wasserstein" with wrong scale)
    
    # Jet-level feature indices in y tensor [jet_type, jet_pt, jet_eta, jet_mass, ...]
    jet_pt_index: 1             # Index of jet_pt in y tensor
    jet_eta_index: 2            # Index of jet_eta in y tensor
    jet_mass_index: 3           # Index of jet_mass in y tensor
    
    # Jet features loss weights (individual MSE components)
    jet_pt_weight: 3.0          # Weight for MSE(jet_pt)
    jet_eta_weight: 0.5         # Weight for MSE(jet_eta)
    jet_mass_weight: 5.0        # Weight for MSE(jet_mass)
    jet_n_constituents_weight: 0.0  # DISABLE: MSE on integer counts has wrong scale (400 >> 1)
    
    # Local→Global Physics Consistency Loss Configuration
    # Enforces agreement between generated particle 4-momenta and true jet-level features
    # Computes jet observables (pT, η, m) from particles and compares to true jet values
    use_consistency_loss: true  # Enable local→global consistency loss
    consistency_pt_weight: 3.0  # Weight for pT residual (most important, affects trigger/selection)
    consistency_eta_weight: 0.5  # Weight for η residual (detector geometry, less critical)
    consistency_mass_weight: 5.0  # Weight for mass residual (physics discriminant, e.g., top/W/Z tagging)
    
    # Distribution Loss: Match edge/hyperedge observable distributions
    use_distribution_loss: true      # Enable distribution matching loss
    distribution_loss_type: "mse"    # Options: "wasserstein" (expensive, CPU), "mse" (fast, GPU)
                                     # Wasserstein: Earth mover's distance (accurate but slow)
    eec_prop: [[2, 3], 200, [1e-4, 2]]  # [[N-point orders], bins, [axis_min, axis_max]]
  
  # KL annealing - Cyclical schedule to prevent posterior collapse and KL explosion
  kl_annealing_schedule: 'cyclical'  # Options: 'linear', 'cyclical'
  kl_warmup_epochs: 15          # INCREASED from 8 - slower warmup
  kl_cycle_epochs: 40           # INCREASED from 25 - longer cycles
  kl_free_bits: 2.0             # INCREASED from 1.5 - more tolerance per dimension
  kl_max_weight: 0.55            # INCREASED from 0.12 - stronger max regularization
                                # Effective max KL weight = 1.0 * 0.5 = 0.5
                                # This should control KL explosion better

  # Gumbel-Softmax Temperature Annealing
  # Controls the sharpness of discrete topology sampling (particle/edge/hyperedge existence)
  # Higher temp → softer/more random, Lower temp → sharper/more deterministic
  # Annealing schedule: temp(epoch) = final_temp + (initial_temp - final_temp) × decay^epoch
  initial_temperature: 10.0       # Start with very soft sampling (explore topology space)
  final_temperature: 0.75         # End with sharp sampling (confident discrete choices)
  temperature_decay: 0.98        # SLOWER from 0.98 - more gradual annealing for stability
  
  # Checkpointing
  save_every: 10
  validate_every: 5
  
  # Early stopping
  patience: 50
  min_delta: 0.001

# Data
data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  shuffle: true
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
