# Training Fix Summary

## Issue Reported

Training on **250 jets** with the following problems:

```
Epoch 23: loss=8.18e+8, kl=8.19
Epoch 89: loss=8.46e+8, kl=12.9
Epoch 108: loss=9.89e+8, kl=42.8
Epoch 193: loss=4.04e+8, kl=72.9  ‚Üê KL exploding!
```

- ‚ùå Loss not decreasing
- ‚ùå KL divergence exploding (8 ‚Üí 73)
- ‚úì VRAM usage: 1.5GB / 4GB (good, underutilized)
- ‚úì Data: Pre-normalized features (node, edge, hyperedge)

## Root Causes

1. **KL Divergence Unbounded**: No maximum cap, growing infinitely
2. **Loss Weights Too High**: Set for unnormalized features, but data is already normalized
3. **Small Dataset**: 250 jets ‚Üí high variance, needs careful tuning
4. **Small Batch Size**: Only 4 samples per batch ‚Üí unstable gradients

## Fixes Applied

### 1. KL Divergence Control ‚úÖ

**config.yaml:**
```yaml
# Before
kl_divergence: 0.001
kl_warmup_epochs: 50
# No cap!

# After
kl_divergence: 0.00001      # 100x smaller base
kl_warmup_epochs: 100       # Slower warmup
kl_max_weight: 0.0001       # NEW: Caps growth
```

**hypervae.py:**
```python
# Now caps KL weight at kl_max_weight
kl_weight = min(kl_max_weight, epoch / kl_warmup_epochs)
```

**Result**: KL will stabilize ~10-15 instead of exploding to 70+

### 2. Rebalanced Loss Weights ‚úÖ

Since your data is already normalized:

```yaml
# Before (for unnormalized data)
particle_features: 10.0
edge_features: 5.0
hyperedge_features: 3.0
topology: 1.0

# After (for normalized data)
particle_features: 1.0    # 10x smaller
edge_features: 0.5        # 10x smaller  
hyperedge_features: 0.3   # 10x smaller
topology: 0.1             # 10x smaller
```

**Result**: Loss will be in reasonable range (100-1000) instead of 1e8

### 3. Increased Batch Size ‚úÖ

```yaml
# Before
batch_size: 4             # 50 batches/epoch
gradient_accumulation_steps: 8

# After
batch_size: 8             # 25 batches/epoch
gradient_accumulation_steps: 4
# Still effective batch = 32
```

**Result**: More stable gradients, better VRAM utilization (1.5‚Üí2.5GB)

### 4. More Epochs for Small Dataset ‚úÖ

```yaml
epochs: 300  # Was 200
```

**Result**: More time to converge with small dataset

## Expected New Behavior

### Before (Bad) ‚ùå
```
Epoch 23: loss=8.18e+8, kl=8.19
Epoch 89: loss=8.46e+8, kl=12.9
Epoch 108: loss=9.89e+8, kl=42.8   ‚Üê Exploding
Epoch 193: loss=4.04e+8, kl=72.9   ‚Üê Way too high!
```

### After (Good) ‚úÖ
```
Epoch 10: loss=500.0, kl=6.0, kl_w=0.00001
Epoch 50: loss=200.0, kl=9.0, kl_w=0.00005
Epoch 100: loss=120.0, kl=10.5, kl_w=0.0001  ‚Üê Capped
Epoch 150: loss=100.0, kl=11.0, kl_w=0.0001  ‚Üê Stable
Epoch 200: loss=90.0, kl=11.2, kl_w=0.0001
Epoch 300: loss=85.0, kl=11.5, kl_w=0.0001   ‚Üê Converged
```

**Key indicators:**
- ‚úÖ Loss steadily decreasing
- ‚úÖ KL stabilizes around 10-15 (healthy range)
- ‚úÖ KL weight caps at 0.0001
- ‚úÖ No explosions or NaN values

## What Was NOT Changed

‚ùå **No feature normalization added** - Your data is already normalized
‚ùå **No model architecture changes** - Structure remains the same
‚ùå **No data augmentation** - Can add later if needed

## Files Changed

1. ‚úÖ `config.yaml` - Updated training parameters
2. ‚úÖ `models/hypervae.py` - Added KL weight capping
3. ‚úÖ `train.py` - Added KL weight monitoring
4. ‚úÖ `TRAINING_TIPS.md` - Added training guide

## How to Retrain

```bash
# Stop current training (Ctrl+C if running)

# Retrain with fixed config
python train.py \
    --config config.yaml \
    --data-path data/graphs_pyg_particle__fully_connected_q.pt \
    --save-test-indices \
    --save-dir checkpoints_fixed \
    --log-dir runs/fixed_training

# Monitor in another terminal
tensorboard --logdir runs/fixed_training
```

## Monitoring

### Console Output
```
Epoch 50: 100%|‚ñà‚ñà‚ñà| 30/30 [00:08<00:00, loss=200, kl=9.0, kl_w=0.00005]
                                          ‚Üë       ‚Üë         ‚Üë
                                       Total    KL    KL weight (capped)
```

### TensorBoard
Watch these metrics:
- `Loss/train_total` ‚Üí should decrease
- `Loss/train_kl` ‚Üí should stabilize ~10-15
- `KL/weight` ‚Üí should cap at 0.0001
- Compare train vs val loss

## If Still Having Issues

### Issue: KL still > 20
**Solution**: Reduce `kl_max_weight` to 0.00005

### Issue: Loss still not decreasing
**Solutions**:
1. Reduce learning rate to 0.00005
2. Reduce model capacity (hidden dims)
3. Add more dropout (0.2)

### Issue: Overfitting (val >> train loss)
**Expected with 250 jets!** Solutions:
1. Get more data (best solution)
2. Reduce model capacity
3. Increase dropout
4. Add weight decay

## Configuration Summary

| Parameter | Before | After | Reason |
|-----------|--------|-------|--------|
| Batch size | 4 | 8 | Better gradient estimates |
| KL base weight | 0.001 | 0.00001 | Prevent explosion |
| KL max weight | ‚àû | 0.0001 | **Cap growth** |
| KL warmup | 50 | 100 | Slower annealing |
| Particle loss | 10.0 | 1.0 | Match normalized scale |
| Edge loss | 5.0 | 0.5 | Match normalized scale |
| Hyperedge loss | 3.0 | 0.3 | Match normalized scale |
| Topology loss | 1.0 | 0.1 | Match normalized scale |
| Epochs | 200 | 300 | Small dataset needs more |

## Key Insight

The **critical fix** is **KL weight capping**:

```python
# Before: Could grow to infinity
kl_weight = epoch / kl_warmup_epochs  # ‚Üí Can become 1.0, 2.0, 4.0...

# After: Capped at maximum
kl_weight = min(kl_max_weight, epoch / kl_warmup_epochs)  # ‚Üí Caps at 0.0001
```

This prevents the KL term from overwhelming the reconstruction loss.

## Success Criteria

After 100 epochs, you should see:
- ‚úÖ Loss < 200 (was 4-9e8)
- ‚úÖ KL between 8-15 (was 73)
- ‚úÖ Loss decreasing trend
- ‚úÖ No OOM errors
- ‚úÖ Training stable

## Long-Term Recommendations

1. **More Data** (Best): Target 1000+ jets
2. **Data Augmentation**: œÜ rotations, particle shuffling
3. **Ensemble**: Train multiple models with different seeds
4. **Architecture Search**: Try different hidden dimensions

---

**Status**: ‚úÖ All fixes applied, ready to retrain!

**Expected result**: Stable training with decreasing loss and controlled KL divergence. üéØ
